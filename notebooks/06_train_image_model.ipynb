{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dc6c1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Negar\\anaconda3\\envs\\crlm\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# 06_train_image_model.ipynb â€” Full EfficientNetB3 model\n",
    "# =============================================================\n",
    "import os\n",
    "os.chdir(r\"C:\\Users\\Negar\\Desktop\\paper_results\\Myself\\cr_coad_project\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea3d15f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "âš ï¸ No GPU detected â€” training will use CPU (much slower).\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Environment setup\n",
    "# -------------------------------------------------------------\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"âœ… GPU detected: {gpus}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected â€” training will use CPU (much slower).\")\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = (300, 300)        # EfficientNetB3 default input\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "SEED = 42\n",
    "\n",
    "DATA_DIR = Path(\"data/processed/images/all_slices\")\n",
    "INDEX = Path(\"data/processed/images/all_index.csv\")\n",
    "SPLIT_DIR = Path(\"data/splits\")\n",
    "CLINICAL_PATH = Path(\"data/processed/clinical/clinical_features_with_id.csv\")\n",
    "SAVE_DIR = Path(\"results/images\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcdd9b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¼ Total image slices: 27822\n",
      "split\n",
      "train    19603\n",
      "test      4166\n",
      "val       4053\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Load datasets + splits\n",
    "# -------------------------------------------------------------\n",
    "df = pd.read_csv(INDEX)\n",
    "train_ids = pd.read_csv(SPLIT_DIR / \"train_series.csv\")[\"series_id\"].tolist()\n",
    "val_ids   = pd.read_csv(SPLIT_DIR / \"val_series.csv\")[\"series_id\"].tolist()\n",
    "test_ids  = pd.read_csv(SPLIT_DIR / \"test_series.csv\")[\"series_id\"].tolist()\n",
    "\n",
    "def assign_split(sid):\n",
    "    if sid in train_ids: return \"train\"\n",
    "    if sid in val_ids:   return \"val\"\n",
    "    if sid in test_ids:  return \"test\"\n",
    "    return \"ignore\"\n",
    "\n",
    "df[\"split\"] = df[\"series_id\"].map(assign_split)\n",
    "df = df[df[\"split\"] != \"ignore\"]\n",
    "\n",
    "print(\"ðŸ–¼ Total image slices:\", len(df))\n",
    "print(df[\"split\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c137441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Labels assigned: 1195 metastatic, 26627 non-metastatic\n",
      "ðŸ“Š Label distribution:\n",
      " label\n",
      "0    26627\n",
      "1     1195\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Merge with clinical metastasis labels\n",
    "# -------------------------------------------------------------\n",
    "clinical = pd.read_csv(CLINICAL_PATH)\n",
    "clinical.columns = clinical.columns.str.strip()\n",
    "meta_col = next((c for c in clinical.columns if \"metastasis\" in c.lower()), None)\n",
    "if meta_col is None:\n",
    "    raise SystemExit(\"âŒ No metastasis column found in clinical data!\")\n",
    "\n",
    "merged = df.merge(clinical[[\"patient_id\", meta_col]], on=\"patient_id\", how=\"left\")\n",
    "\n",
    "possible_cols = [c for c in merged.columns if \"metastasis\" in c.lower()]\n",
    "meta_col_final = possible_cols[0]\n",
    "merged = merged.rename(columns={meta_col_final: \"metastasis_status\"})\n",
    "\n",
    "merged[\"label\"] = merged[\"metastasis_status\"].replace({\n",
    "    1: 1, 0: 0,\n",
    "    \"yes\": 1, \"metastatic\": 1, \"positive\": 1,\n",
    "    \"no\": 0, \"non-metastatic\": 0, \"negative\": 0\n",
    "})\n",
    "merged[\"label\"] = pd.to_numeric(merged[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "print(f\"âœ… Labels assigned: {merged['label'].sum()} metastatic, {len(merged)-merged['label'].sum()} non-metastatic\")\n",
    "print(\"ðŸ“Š Label distribution:\\n\", merged['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4595578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datasets ready â€” Train: 19603, Val: 4053, Test: 4166\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# TensorFlow dataset builders\n",
    "# -------------------------------------------------------------\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def decode_img(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    return tf.cast(img, tf.float32) / 255.0\n",
    "\n",
    "def make_dataset(subdf, augment=False, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((subdf[\"slice_path\"].values, subdf[\"label\"].values))\n",
    "    ds = ds.map(lambda p, y: (decode_img(p), y), num_parallel_calls=AUTOTUNE)\n",
    "    if augment:\n",
    "        ds = ds.map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
    "        ds = ds.map(lambda x, y: (tf.image.random_brightness(x, 0.15), y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(2048, seed=SEED)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "train_ds = make_dataset(merged[merged.split==\"train\"], augment=True)\n",
    "val_ds   = make_dataset(merged[merged.split==\"val\"])\n",
    "test_ds  = make_dataset(merged[merged.split==\"test\"], shuffle=False)\n",
    "\n",
    "print(f\"âœ… Datasets ready â€” Train: {len(merged[merged.split=='train'])}, Val: {len(merged[merged.split=='val'])}, Test: {len(merged[merged.split=='test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Clear session before building a big model\n",
    "# -------------------------------------------------------------\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a291fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
      "43941136/43941136 [==============================] - 12s 0us/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 300, 300, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetb3 (Functional)  (None, 1536)             10783535  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1536)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 1537      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,785,072\n",
      "Trainable params: 10,697,769\n",
      "Non-trainable params: 87,303\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Model definition â€” EfficientNetB3\n",
    "# -------------------------------------------------------------\n",
    "base = tf.keras.applications.EfficientNetB3(\n",
    "    include_top=False,\n",
    "    input_shape=IMG_SIZE + (3,),\n",
    "    weights=\"imagenet\",\n",
    "    pooling=\"avg\"\n",
    ")\n",
    "base.trainable = True   # fine-tune all layers (can freeze first N later)\n",
    "\n",
    "inputs = layers.Input(shape=IMG_SIZE + (3,))\n",
    "x = base(inputs, training=True)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.AUC(name=\"auc\"),\n",
    "        tf.keras.metrics.Precision(name=\"precision\"),\n",
    "        tf.keras.metrics.Recall(name=\"recall\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06439284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Callbacks (with JSON-safe and weight-only checkpoints)\n",
    "# -------------------------------------------------------------\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def safe_float(x):\n",
    "    \"\"\"Convert any metric value (tensor/ndarray) to plain float.\"\"\"\n",
    "    try:\n",
    "        if hasattr(x, \"numpy\"):\n",
    "            x = x.numpy()\n",
    "        if isinstance(x, (np.ndarray, list, tuple)):\n",
    "            return float(np.mean(x))\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "class SafeJSONCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Prevent JSON serialization errors in Keras history.\"\"\"\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs:\n",
    "            for k, v in logs.items():\n",
    "                logs[k] = safe_float(v)\n",
    "\n",
    "class SafeReduceLROnPlateau(tf.keras.callbacks.ReduceLROnPlateau):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = {k: safe_float(v) for k, v in (logs or {}).items()}\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "# Weight-only checkpoint file (robust across TF versions)\n",
    "checkpoint_path = SAVE_DIR / \"best_image_model.weights.h5\"\n",
    "\n",
    "callbacks = [\n",
    "    SafeJSONCallback(),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_auc\", patience=5, mode=\"max\", restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=str(checkpoint_path),\n",
    "        monitor=\"val_auc\", save_best_only=True,\n",
    "        save_weights_only=True,   # âœ… prevents broken .keras files\n",
    "        mode=\"max\", verbose=1),\n",
    "    SafeReduceLROnPlateau(\n",
    "        monitor=\"val_auc\", factor=0.3, patience=3, verbose=1, mode=\"max\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Resume from checkpoint if available\n",
    "# -------------------------------------------------------------\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"ðŸ” Found existing checkpoint at {checkpoint_path}, resuming training (loading weights only)...\")\n",
    "    model.load_weights(checkpoint_path)\n",
    "else:\n",
    "    print(\"ðŸš€ Starting new training run from scratch...\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Determine last completed epoch\n",
    "# -------------------------------------------------------------\n",
    "history_path = SAVE_DIR / \"training_history_best.csv\"\n",
    "initial_epoch = 0\n",
    "if history_path.exists():\n",
    "    hist = pd.read_csv(history_path)\n",
    "    initial_epoch = len(hist)\n",
    "    print(f\"â© Resuming training from epoch {initial_epoch} of {EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting full model training...\n",
      "Epoch 1/30\n",
      "1017/1226 [=======================>......] - ETA: 48:30 - loss: 0.0181 - accuracy: 0.9950 - auc: 0.9958 - precision: 0.9255 - recall: 0.9305"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸš€ Starting full model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Negar\\anaconda3\\envs\\crlm\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Negar\\anaconda3\\envs\\crlm\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Negar\\anaconda3\\envs\\crlm\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Negar\\anaconda3\\envs\\crlm\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Negar\\anaconda3\\envs\\crlm\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Negar\\anaconda3\\envs\\crlm\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Negar\\anaconda3\\envs\\crlm\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Negar\\anaconda3\\envs\\crlm\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Negar\\anaconda3\\envs\\crlm\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Train\n",
    "# -------------------------------------------------------------\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    initial_epoch=initial_epoch,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fadff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Evaluate\n",
    "# -------------------------------------------------------------\n",
    "results = model.evaluate(test_ds, return_dict=True)\n",
    "print(\"\\nâœ… Final Test Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "pd.DataFrame(history.history).to_csv(SAVE_DIR / \"training_history_best.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Save model and embeddings\n",
    "# -------------------------------------------------------------\n",
    "print(\"\\nðŸ’¾ Saving model and feature embeddings...\")\n",
    "\n",
    "# Save in pure TensorFlow SavedModel format (robust & safe)\n",
    "tf.saved_model.save(model, str(SAVE_DIR / \"baseline_image_model_full_tf\"))\n",
    "print(\"âœ… Model saved successfully (TensorFlow SavedModel format).\")\n",
    "\n",
    "# Extract feature embeddings\n",
    "feature_extractor = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "all_paths = merged[\"slice_path\"].tolist()\n",
    "batch_size = 32\n",
    "embeddings = []\n",
    "\n",
    "for i in range(0, len(all_paths), batch_size):\n",
    "    batch_imgs = [decode_img(p) for p in all_paths[i:i+batch_size]]\n",
    "    batch_tensor = tf.stack(batch_imgs)\n",
    "    emb = feature_extractor(batch_tensor).numpy()\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "np.save(SAVE_DIR / \"image_embeddings_full.npy\", embeddings)\n",
    "pd.DataFrame({\n",
    "    \"slice_path\": all_paths,\n",
    "    \"label\": merged[\"label\"].tolist()\n",
    "}).to_csv(SAVE_DIR / \"image_embeddings_index_full.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Embeddings saved: {embeddings.shape} â†’ {SAVE_DIR/'image_embeddings_full.npy'}\")\n",
    "print(\"ðŸŽ¯ Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
